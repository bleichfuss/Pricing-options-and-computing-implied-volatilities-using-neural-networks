{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw1fGnfYE_oI"
      },
      "source": [
        "# Forward pass\n",
        "\n",
        "# from \"A neural network-based framework for financial model calibration \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vrrvyBwafo8",
        "outputId": "c27addc2-9dc4-4b05-f49c-75e535014882"
      },
      "outputs": [],
      "source": [
        "# Basic imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "# Updating scipy to use module QMC for Latin hypercube sampling\n",
        "#!pip install scipy==1.7\n",
        "from scipy.stats import qmc\n",
        "\n",
        "# Pytorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR, LinearLR, OneCycleLR, LambdaLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hP3TMaNajHA"
      },
      "outputs": [],
      "source": [
        "def bs_price_call(S, K, T, t, r, sigma):\n",
        "  \"\"\"\n",
        "  Black-Scholes call option value price related to PDE (2). Returns option's price.\n",
        "  S: spot price\n",
        "  K: strike price\n",
        "  T: time to maturity\n",
        "  t: spot time\n",
        "  r: interest rate\n",
        "  sigma: volatility of underlying asset\n",
        "  \"\"\"\n",
        "\n",
        "  # Boundary condition of Black-Sholes PDE (2)\n",
        "  if T == t:\n",
        "    return np.maximum(0, S - K)  \n",
        "  # Solution of the PDE (2) on the open interval [t, T)\n",
        "  else : \n",
        "    d1 = ( np.log(S / K) + (r + 0.5 * sigma ** 2) * (T-t) ) / (sigma * np.sqrt(T-t))\n",
        "    d2 = d1 - sigma*np.sqrt(T-t)\n",
        "    value = S * norm.cdf(d1) - K * np.exp(-r * (T-t)) * norm.cdf(d2) \n",
        "  \n",
        "  return value\n",
        "\n",
        "def bs_moneyness_call(moneyness, tau, r, sigma):\n",
        "  \"\"\"\n",
        "  Same function as bs_price_call but in terms of moneyness. Returns V/K and \n",
        "  has S/K instead of two seperate paremeters S and K.\n",
        "  moneyness: S/K\n",
        "  Tau: time to maturity minus spot time\n",
        "  r: interest rate\n",
        "  sigma: volatility of underlying asset\n",
        "  \"\"\"\n",
        "\n",
        "  d1 = ( np.log(moneyness) + (r + 0.5 * sigma ** 2) * (tau) ) / (sigma * np.sqrt(tau))\n",
        "  d2 = d1 - sigma*np.sqrt(tau)\n",
        "  value_moneyness = moneyness * norm.cdf(d1) - np.exp(-r * (tau)) * norm.cdf(d2) \n",
        "  \n",
        "  return value_moneyness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdhtgoThB8B3"
      },
      "source": [
        "### Data generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNTO4M51R3cd"
      },
      "source": [
        "### Defining class BS-ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc26TY2fYdoa"
      },
      "outputs": [],
      "source": [
        "class BS_ANN(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initilize BS-ANN netwwork with four hidden layers. \n",
        "    Same architecture as the one used in table 2.\n",
        "    Weight are initialized using Glorot_uniform also known as Xavier_uniform.\n",
        "    \"\"\"\n",
        "    super(BS_ANN, self).__init__()\n",
        "    # 4 layers\n",
        "    self.fc1 = nn.Linear(4, 400)\n",
        "    self.fc2 = nn.Linear(400, 400)\n",
        "    self.fc3 = nn.Linear(400, 400)\n",
        "    self.fc4 = nn.Linear(400, 400)\n",
        "    self.fc5 = nn.Linear(400, 1)\n",
        "    # initilizing the layer's weights\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.fc5.weight)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Foward pass of the network. \n",
        "    Relu activations are used and a linear activation is used in the final layer.\n",
        "    x: input torch tensor (CUDA or CPU)\n",
        "    \"\"\"\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = self.fc5(x)\n",
        "    return x.flatten()\n",
        "\n",
        "  def train_model(self, args, device, train_loader, optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Training over train_loader batches for one epoch.\n",
        "    Returns average training loss.\n",
        "    args: training parameters.\n",
        "    device: CUDA or CPU.\n",
        "    train_loader: train data loader.\n",
        "    optimizer: gradient descent optmizer.\n",
        "    epoch: current epoch.\n",
        "    \"\"\"\n",
        "    # train mode\n",
        "    self.train()\n",
        "    train_loss = 0\n",
        "    # loop over data loader\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "      # extracting the inputs and target\n",
        "      data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
        "      # zeroing the gradients\n",
        "      optimizer.zero_grad()\n",
        "      # foward pass\n",
        "      output = self(data)\n",
        "      # computing the loss\n",
        "      loss = F.mse_loss(output, target)\n",
        "      train_loss += loss.item() * data.shape[0]\n",
        "      # back propagation\n",
        "      loss.backward()\n",
        "      # updating the optimizer step\n",
        "      optimizer.step()\n",
        "      # printing batch information\n",
        "      if batch_idx % args[\"log_interval\"] == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlog Loss: {:.6f}'.format(\n",
        "            epoch, int(batch_idx / len(train_loader)*len(train_loader.dataset)), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), np.log(loss.item())))\n",
        "        if args[\"dry_run\"]:\n",
        "          break\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    print('\\nTrain set: Average log loss: {:.4f}\\n'.format(np.log(train_loss))) \n",
        "    return train_loss\n",
        "\n",
        "\n",
        "  def test_model(self, device, test_loader):\n",
        "    \"\"\"\n",
        "    Testing over test_loader batches for one epoch. \n",
        "    Returns average test loss.\n",
        "    device: CUDA or CPU.\n",
        "    test_loader: test data loader.\n",
        "    \"\"\"\n",
        "    # evaluation mode\n",
        "    self.eval()\n",
        "    test_loss = 0\n",
        "    # freezing the gradients\n",
        "    with torch.no_grad():\n",
        "      # looping over the test laoder\n",
        "      for data in test_loader:\n",
        "        # extracting the inputs and target\n",
        "        data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
        "        # foward pass\n",
        "        output = self(data)\n",
        "        #computing the loss\n",
        "        test_loss += F.mse_loss(output, target, reduction = 'sum').item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}\\n'.format(np.log(test_loss))) \n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3QMaOLPPwux"
      },
      "source": [
        "# 3) COS Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBQ-dVz4DFUh"
      },
      "source": [
        "#### Vanilla call coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEFZmb6MBYVq"
      },
      "source": [
        "$\\begin{aligned} \\chi_{k}(c, d):=& \\frac{1}{1+\\left(\\frac{k \\pi}{b-a}\\right)^{2}}\\left[\\cos \\left(k \\pi \\frac{d-a}{b-a}\\right) e^{d}-\\cos \\left(k \\pi \\frac{c-a}{b-a}\\right) e^{c}\\right.\\\\ &\\left.+\\frac{k \\pi}{b-a} \\sin \\left(k \\pi \\frac{d-a}{b-a}\\right) e^{d}-\\frac{k \\pi}{b-a} \\sin \\left(k \\pi \\frac{c-a}{b-a}\\right) e^{c}\\right] \\end{aligned}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UYjpY3wBabR"
      },
      "source": [
        "$\\psi_{k}(c, d):= \\begin{cases}{\\left[\\sin \\left(k \\pi \\frac{d-a}{b-a}\\right)-\\sin \\left(k \\pi \\frac{c-a}{b-a}\\right)\\right] \\frac{b-a}{k \\pi},} & k \\neq 0, \\\\ (d-c), & k=0 .\\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aP1rhLNBrIm"
      },
      "source": [
        "$V_{k}^{\\text {call }}=\\frac{2}{b-a} K\\left(\\chi_{k}(0, b)-\\psi_{k}(0, b)\\right)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zjZDCl9_SeJ"
      },
      "outputs": [],
      "source": [
        "def call_cosine_coef(a, b, c, d, K, N):\n",
        "  \"\"\"\n",
        "  Coefficient V_k for plain vanilla options. \n",
        "  Section 3.1, equations (22) and (23).\n",
        "  a, b, c, d: truncation range parameters.\n",
        "  K: Strike.\n",
        "  N: summing lenght.\n",
        "  Returns V_k coefficient for call option.\n",
        "  \"\"\"\n",
        "  k = np.arange(N).reshape(1,-1)\n",
        "  bma    = b - a\n",
        "  uu     = k * np.pi / bma\n",
        "  chi_k = 1 / (1 + uu **2 ) * \\\n",
        "  ( np.cos(uu * ( d - a))* np.exp(d) - np.cos(uu * (c - a))* np.exp(c) + uu * np.sin(uu * (d - a)) * np.exp(d)- uu * np.sin(uu*(c-a))* np.exp(c) )\n",
        "  psi_k = uu\n",
        "  psi_k[:,1:] = 1 / uu[:,1:] * ( np.sin(uu[:,1:] * ( d - a )) - np.sin( uu[:,1:] * ( c - a) ) )\n",
        "  psi_k[:,0] = (d - c).flatten() #d - c \n",
        "  return 2/bma*K*(chi_k-psi_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsi05vUVbzTK"
      },
      "outputs": [],
      "source": [
        "def call_cosine_coef_moy(a, b, c, d, N):\n",
        "  \"\"\"\n",
        "  Moyeness version.\n",
        "  Coefficient V_k for plain vanilla options. \n",
        "  Section 3.1, equations (22) and (23).\n",
        "  a, b, c, d: truncation range parameters.\n",
        "  N: summing lenght.\n",
        "  Returns V_k coefficient for call option devided by K.\n",
        "  \"\"\"\n",
        "  k = np.arange(N).reshape(1,-1)\n",
        "  bma    = b - a\n",
        "  uu     = k * np.pi / bma\n",
        "  chi_k = 1 / (1 + uu **2 ) * \\\n",
        "  ( np.cos(uu * ( d - a))* np.exp(d) - np.cos(uu * (c - a))* np.exp(c) + uu * np.sin(uu * (d - a)) * np.exp(d)- uu * np.sin(uu*(c-a))* np.exp(c) )\n",
        "  psi_k = uu\n",
        "  psi_k[:,1:] = 1 / uu[:,1:] * ( np.sin(uu[:,1:] * ( d - a )) - np.sin( uu[:,1:] * ( c - a) ) )\n",
        "  psi_k[:,0] = (d - c).flatten() #d - c \n",
        "  return 2/bma*(chi_k-psi_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py6auAo1DAq3"
      },
      "source": [
        "#### Charateristic function for Heston model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgOVOvZUB2jG"
      },
      "source": [
        "$\\begin{aligned} \\varphi_{h e s}\\left(\\omega ; u_{0}\\right)=& \\exp \\left(i \\omega \\mu \\Delta t+\\frac{u_{0}}{\\eta^{2}}\\left(\\frac{1-e^{-D \\Delta t}}{1-G e^{-D \\Delta t}}\\right)(\\lambda-i \\rho \\eta \\omega-D)\\right) \\\\ & \\cdot \\exp \\left(\\frac{\\lambda \\bar{u}}{\\eta^{2}}\\left(\\Delta t(\\lambda-i \\rho \\eta \\omega-D)-2 \\log \\left(\\frac{1-G e^{-D \\Delta t}}{1-G}\\right)\\right)\\right) \\end{aligned}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Wi_OJIB3o4"
      },
      "source": [
        "$D=\\sqrt{(\\lambda-i \\rho \\eta \\omega)^{2}+\\left(\\omega^{2}+i \\omega\\right) \\eta^{2}} \\quad$ and $\\quad G=\\frac{\\lambda-i \\rho \\eta \\omega-D}{\\lambda-i \\rho \\eta \\omega+D} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwbqZhhC_Se9"
      },
      "outputs": [],
      "source": [
        "def phi_Heston(a ,b ,N ,r, tau, kappa, gamma, vbar, v0, rho):\n",
        "  \"\"\"\n",
        "  Characteristic function of the log-asset price following the Heston model.\n",
        "  Section 3.3, equation (33).\n",
        "  a, b: truncation range parameters.\n",
        "  N: summing lenght.\n",
        "  r: risk free rate.\n",
        "  tau: time to maturity.\n",
        "  kappa: reversion speed .\n",
        "  gamma: Vol of vol.\n",
        "  vbar: Long average variance.\n",
        "  v0: Initial variance.\n",
        "  rho: Correlation.\n",
        "  Returns Charateristic function vector.\n",
        "  \"\"\"\n",
        "  k = np.arange(N).reshape(1,-1)\n",
        "  u = np.pi*k/(b-a)\n",
        "  i = complex(0.0,1.0)\n",
        "  D = np.sqrt((kappa-gamma*rho*i*u)**2+(u**2+i*u)*gamma**2)\n",
        "  G = (kappa-gamma*rho*i*u-D)/(kappa-gamma*rho*i*u+D)\n",
        "  C = v0* (1-np.exp(-D*tau)) / (gamma**2*(1-G*np.exp(-D*tau)))*(kappa-gamma*rho*i*u-D)\n",
        "  A = r*i*u*tau + kappa*vbar/(gamma**2)*(tau*(kappa-gamma*rho*i*u-D) -2*np.log( (1-G*np.exp(-D*tau))/(1-G) )   ) \n",
        "  phi = np.exp( A + C )\n",
        "  return phi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF6h1RohC9KX"
      },
      "source": [
        "#### Heston cumulants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyNS4d1TCIBI"
      },
      "source": [
        "$c_{1}=\\mu T+\\left(1-e^{-\\lambda T}\\right) \\frac{\\bar{u}-u_{0}}{2 \\lambda}-\\frac{1}{2} \\bar{u} T$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIcIQxmcCRDU"
      },
      "source": [
        "$\\begin{aligned} c_{2}=& \\frac{1}{8 \\lambda^{3}}\\left(\\eta T \\lambda e^{-\\lambda T}\\left(u_{0}-\\bar{u}\\right)(8 \\lambda \\rho-4 \\eta)\\right.\\\\ &+\\lambda \\rho \\eta\\left(1-e^{-\\lambda T}\\right)\\left(16 \\bar{u}-8 u_{0}\\right) \\\\ &+2 \\bar{u} \\lambda T\\left(-4 \\lambda \\rho \\eta+\\eta^{2}+4 \\lambda^{2}\\right) \\\\ &+\\eta^{2}\\left(\\left(\\bar{u}-2 u_{0}\\right) e^{-2 \\lambda T}+\\bar{u}\\left(6 e^{-\\lambda T}-7\\right)+2 u_{0}\\right) \\\\ &\\left.+8 \\lambda^{2}\\left(u_{0}-\\bar{u}\\right)\\left(1-e^{-\\lambda T}\\right)\\right) \\end{aligned}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTCdkZDZ_Sid"
      },
      "outputs": [],
      "source": [
        "def Heston_cumulants( r, tau, kappa, gamma, vbar, v0, rho ):\n",
        "  \"\"\"\n",
        "  Computes Heston cumulants as defined in the appendix A for the Heston model.\n",
        "  r: risk free rate.\n",
        "  tau: time to maturity.\n",
        "  kappa: reversion speed .\n",
        "  gamma: Vol of vol.\n",
        "  vbar: Long average variance.\n",
        "  v0: Initial variance.\n",
        "  rho: Correlation.\n",
        "  Returns cumulants c1 and c2.\n",
        "  \"\"\"\n",
        "  c1 = r * tau + (1 - np.exp(- kappa * tau)) * (vbar - v0) / (2 * kappa) - 0.5 * vbar * tau\n",
        "  \n",
        "  c2 = 0.125 / kappa ** 3 * (gamma * tau * np.exp(- kappa * tau) * (v0 - vbar) * (8 * kappa * rho - 4 * gamma) + \\\n",
        "     kappa * rho * gamma * (1 - np.exp(- kappa * tau)) * (16 * vbar - 8 * v0) + \\\n",
        "     2 * vbar * kappa * tau * (- 4 * kappa * rho * gamma + gamma ** 2 + 4 * kappa ** 2) + \\\n",
        "     gamma ** 2 * ((vbar - 2 * v0) * np.exp(-2 * kappa * tau) + vbar * (6 * np.exp(- kappa * tau) - 7) + 2 * v0) + \\\n",
        "     8 * kappa ** 2 * (v0 - vbar) * (1 - np.exp(-kappa * tau)))\n",
        "  return c1, c2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fO7JQlDDNcW"
      },
      "source": [
        "#### Truncation range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Uqcq59ChoT"
      },
      "source": [
        "$[a, b]:=\\left[c_{1}-L \\sqrt{c_{2}+\\sqrt{c_{4}}}, \\quad c_{1}+L \\sqrt{c_{2}+\\sqrt{c_{4}}}\\right]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjrTS6X1FqqF"
      },
      "outputs": [],
      "source": [
        "def Truncation_range(c1, c2, c4, L):\n",
        "  \"\"\"\n",
        "  Computes the truncation range given the cumulants c1, c2, c4 and the truncation lenght L.\n",
        "  c1, c2, c4: Model cumulants.\n",
        "  L: truncation lenght.\n",
        "  Returns tuncation interval sup b and inf a.\n",
        "  \"\"\"\n",
        "  a = c1 - L * np.sqrt( np.abs( c2 ) + np.sqrt( np.abs(c4) ) )\n",
        "  b = c1 + L * np.sqrt( np.abs( c2 ) + np.sqrt( np.abs(c4) ) ) \n",
        "  return a, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehFOGXCpDPyX"
      },
      "source": [
        "#### Exponential coefficient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZul_Sr-Cof5"
      },
      "source": [
        "$e^{i k \\pi \\frac{\\mathbf{x}-a}{b-a}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2emRWahI3bJ"
      },
      "outputs": [],
      "source": [
        "def exponential_coef(a, b, N, S0, K):\n",
        "  \"\"\"\n",
        "  Computes the exponential coefficient used in the final pricing sum.\n",
        "  a, b: truncation range.\n",
        "  N: sum lenght.\n",
        "  S0: asset initial price.\n",
        "  K: strike.\n",
        "  Returns the exponential coefficient vector.\n",
        "  \"\"\"\n",
        "  i = complex(0.0,1.0)\n",
        "  k = np.arange(N).reshape(1,-1)\n",
        "  return np.exp(i*k*np.pi*(np.log(S0/K)-a)/(b-a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLSrDystbZ6F"
      },
      "outputs": [],
      "source": [
        "def exponential_coef_moy(a, b, N, moyeness):\n",
        "  \"\"\"\n",
        "  Moyeness version.\n",
        "  Computes the exponential coefficient used in the final pricing sum.\n",
        "  a, b: truncation range.\n",
        "  N: sum lenght.\n",
        "  moyeness: S/K.\n",
        "  Returns the exponential coefficient vector devided by K.\n",
        "  \"\"\"\n",
        "  i = complex(0.0,1.0)\n",
        "  k = np.arange(N).reshape(1,-1)\n",
        "  return np.exp(i*k*np.pi*(np.log(moyeness)-a)/(b-a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfm4WDluDS_2"
      },
      "source": [
        "#### Vanilla call price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEAn4EYbCttc"
      },
      "source": [
        "$v\\left(\\mathbf{x}, t_{0}, u_{0}\\right) \\approx \\mathbf{K} e^{-r \\Delta t} \\cdot \\operatorname{Re}\\left\\{\\sum_{k=0}^{N-1} \\varphi_{h e s}\\left(\\frac{k \\pi}{b-a} ; u_{0}\\right) U_{k} \\cdot e^{i k \\pi \\frac{\\mathbf{x}-a}{b-a}}\\right\\}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgdAg_ts_SjW"
      },
      "outputs": [],
      "source": [
        "def call_option_Heston(N, L, r, tau, kappa, gamma, vbar, v0, rho, K, S0 ):\n",
        "  \"\"\"\n",
        "  Computes call option price using COS method.\n",
        "  N: sum lenght.\n",
        "  L: truncation lenght.\n",
        "  r: risk free rate.\n",
        "  tau: time to maturity.\n",
        "  kappa: reversion speed .\n",
        "  gamma: Vol of vol.\n",
        "  vbar: Long average variance.\n",
        "  v0: Initial variance.\n",
        "  rho: Correlation.\n",
        "  S0: asset initial price.\n",
        "  K: strike.\n",
        "  Returns option price.\n",
        "  \"\"\"\n",
        "  c1, c2 = Heston_cumulants( r, tau, kappa, gamma, vbar, v0, rho )\n",
        "  c4 = 0\n",
        "  a, b = Truncation_range(c1, c2, c4, L)\n",
        "  c= 0\n",
        "  d = b\n",
        "  V_k = call_cosine_coef(a, b, c, d, K, N)\n",
        "  phi_k = phi_Heston(a, b, N, r, tau, kappa, gamma, vbar, v0, rho)\n",
        "  product_k = V_k * phi_k * exponential_coef(a, b, N, S0, K)\n",
        "  product_k[:,0] = .5 * product_k[:,0]\n",
        "  return np.exp(-r*tau)*(np.real(  np.sum( product_k,1 )  )).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vnE4AhoTPY5",
        "outputId": "175e8b15-8109-49a6-faf1-a3c1a4217b30"
      },
      "outputs": [],
      "source": [
        "# cheking the function\n",
        "N = 500\n",
        "L= 12\n",
        "r= .1\n",
        "tau= 1.4\n",
        "rho = -0.95\n",
        "kappa= 1.4\n",
        "vbar = .1\n",
        "gamma = .1\n",
        "v0 = 0.5\n",
        "K = 100\n",
        "S0 = 100\n",
        "# Feller's condition\n",
        "print(2*kappa*vbar>gamma**2)\n",
        "call_option_Heston(N = 1500, L= 12, r= r, tau= tau, kappa= kappa, gamma = gamma, vbar = vbar, v0 = v0, rho = rho, K = K, S0 = S0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4xZ6hb-bgph"
      },
      "outputs": [],
      "source": [
        "def call_option_Heston_moy(N, L, r, tau, kappa, gamma, vbar, v0, rho, moyeness ):\n",
        "  \"\"\"\n",
        "  Moyeness version.\n",
        "  Computes call option price using COS method.\n",
        "  N: sum lenght.\n",
        "  L: truncation lenght.\n",
        "  r: risk free rate.\n",
        "  tau: time to maturity.\n",
        "  kappa: reversion speed .\n",
        "  gamma: Vol of vol.\n",
        "  vbar: Long average variance.\n",
        "  v0: Initial variance.\n",
        "  rho: Correlation.\n",
        "  moyeness: S/K.\n",
        "  Returns option price.\n",
        "  \"\"\"\n",
        "  c1, c2 = Heston_cumulants( r, tau, kappa, gamma, vbar, v0, rho )\n",
        "  c4 = 0\n",
        "  a, b = Truncation_range(c1, c2, c4, L)\n",
        "  c= 0\n",
        "  d = b\n",
        "  V_k = call_cosine_coef_moy(a, b, c, d, N)\n",
        "  phi_k = phi_Heston(a, b, N, r, tau, kappa, gamma, vbar, v0, rho)\n",
        "  product_k = V_k * phi_k * exponential_coef_moy(a, b, N, moyeness)\n",
        "  product_k[:,0] = .5 * product_k[:,0]\n",
        "  return np.exp(-r*tau)*(np.real(  np.sum( product_k,1 )  )).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm7u10L0cDYL",
        "outputId": "c896e876-d916-441a-c8a6-4348e918a528"
      },
      "outputs": [],
      "source": [
        "# cheking the function\n",
        "N = 500\n",
        "L= 12\n",
        "r= .1\n",
        "tau= 1.4\n",
        "rho = -0.95\n",
        "kappa= 1.4\n",
        "vbar = .1\n",
        "gamma = .1\n",
        "v0 = 0.5\n",
        "K = 100\n",
        "S0 = 100\n",
        "# Feller's condition\n",
        "print(2*kappa*vbar>gamma**2)\n",
        "call_option_Heston_moy(N = 1500, L= 12, r= r, tau= tau, kappa= kappa, gamma = gamma, vbar = vbar, v0 = v0, rho = rho, moyeness = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aCVlS1x_RIt"
      },
      "source": [
        "# 4) Heston pricing network: Heston-ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JZsZfYFUDjZ"
      },
      "source": [
        "### Generating Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZC85E7uUGor",
        "outputId": "17b0b197-5a7c-425f-acbe-bffc2f996333"
      },
      "outputs": [],
      "source": [
        "# Table 3 of Liu et al.\n",
        "# moneyness m =\n",
        "eps = 1e-2 # is used to generate open intervals\n",
        "def Heston_LHS_data_generator(n = 10**6, l_bounds = [.6+eps, .05+eps, .0+eps, -0.9+eps, 0.4+eps, .01+eps, .01+eps, .05+eps], u_bounds = [1.4-eps, 3.0-eps, .05-eps, 0.0-eps, 3.0-eps, .5-eps, .8-eps, .5-eps]):\n",
        "  \"\"\"\n",
        "  Generates samples of call prices using Latin hypercube sampling.\n",
        "  Returns a torch float tensor of dimension (n,9) containig the inputs samples and the value of the call.\n",
        "  The inputs are: Moyeness (S/K), time to maturity (tau), Risk free rate (r), Correlation (rho), reversion speed (kappa), Long average variance (vbar), Vol of vol (gamma), Initial variance (v0).\n",
        "  n: number of samples.\n",
        "  l_bounds: lower bound for the inputs.  \n",
        "  u_bounds: upper bound for the inputs.\n",
        "  \"\"\"\n",
        "  sampler = qmc.LatinHypercube(d = 8) #, seed = 0\n",
        "  sample = sampler.random(n)\n",
        "  sample = qmc.scale(sample, l_bounds, u_bounds)\n",
        "  indecies = np.argwhere(2*sample[:,4]*sample[:,5] <= sample[:,6]**2)\n",
        "  sample = np.delete(sample, indecies, 0)\n",
        "  hs_prices = call_option_Heston_moy(N = 160, L= 12, r= sample[:,2].reshape((-1,1)), tau= sample[:,1].reshape((-1,1)), kappa = sample[:,4].reshape((-1,1)),\\\n",
        "                                     gamma = sample[:,6].reshape((-1,1)), vbar = sample[:,5].reshape((-1,1)), v0 = sample[:,7].reshape((-1,1)),\\\n",
        "                                     rho = sample[:,3].reshape((-1,1)), moyeness = sample[:,0].reshape((-1,1))).reshape((-1,1))\n",
        "  # hs_prices = call_option_Heston_moy(N = 1500, L= 50, r= sample[:,2].reshape((-1,1)), tau= sample[:,1].reshape((-1,1)), kappa= sample[:,4].reshape((-1,1)),\\\n",
        "  #                                    gamma = sample[:,6].reshape((-1,1)), vbar = sample[:,5].reshape((-1,1)), v0 = sample[:,7].reshape((-1,1)),\\\n",
        "  #                                    rho = sample[:,3].reshape((-1,1)), moyeness = sample[:,0].reshape((-1,1))).reshape((-1,1))\n",
        "\n",
        "  hs_dataset = np.concatenate((sample, hs_prices),axis=1)\n",
        "\n",
        "  print(\"------ INPUT ------\")\n",
        "  print(\"S/K.......... [{:.2f} {:.2f}]\".format(min(hs_dataset[:,0]), max(hs_dataset[:,0])))\n",
        "  print(\"Tau.......... [{:.2f} {:.2f}]\".format(min(hs_dataset[:,1]), max(hs_dataset[:,1])))\n",
        "  print(\"r............ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,2]), max(hs_dataset[:,2])))\n",
        "  print(\"Corr ........ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,3]), max(hs_dataset[:,3])))\n",
        "  print(\"Kappa........ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,4]), max(hs_dataset[:,4])))\n",
        "  print(\"V_bar........ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,5]), max(hs_dataset[:,5])))\n",
        "  print(\"Gamma........ [{:.2f} {:.2f}]\".format(min(hs_dataset[:,6]), max(hs_dataset[:,6])))\n",
        "  print(\"V_0 ......... [{:.2f} {:.2f}]\".format(min(hs_dataset[:,7]), max(hs_dataset[:,7])))\n",
        "  print(\"------ OUTPUT ------\")\n",
        "  print(\"V/K.......... [{:.2f} {:.2f}]\".format(min(hs_dataset[:,-1]), max(hs_dataset[:,-1])))\n",
        "  # ind_max = np.argmax(hs_dataset[:,-1])\n",
        "  # print(hs_dataset[ind_max,:])\n",
        "\n",
        "  return torch.FloatTensor(hs_dataset)\n",
        "\n",
        "hs_dataset = Heston_LHS_data_generator(10**4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOjnH0N6HMXt"
      },
      "source": [
        "### Defining class Heston-ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nQmv3BmUGpg"
      },
      "outputs": [],
      "source": [
        "class Heston_ANN(BS_ANN):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initilize BS-Heston_ANN netwwork. Same architecture as the one used in tge article.\n",
        "    Weight are initialized using Glorot_uniform also known as Xavier_uniform.\n",
        "    Takes inputs of size 8.\n",
        "    \"\"\"\n",
        "    super(Heston_ANN, self).__init__()\n",
        "    self.fc1 = nn.Linear(8, 200)\n",
        "    self.fc2 = nn.Linear(200, 200)\n",
        "    self.fc3 = nn.Linear(200, 200)\n",
        "    self.fc4 = nn.Linear(200, 200)\n",
        "    self.fc5 = nn.Linear(200, 1)\n",
        "\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.fc5.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3o5w0J_Hl1O"
      },
      "source": [
        "### Training Heston-ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh1i0WibUGuD",
        "outputId": "19eb2734-d269-47d7-bd8e-476a390180d9"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "args =  {\"batch_size\": 1024,\n",
        "         \"test_batch_size\": 4048,\n",
        "         \"epochs\" : 8*10**3,\n",
        "         \"lr\": 1e-4,\n",
        "         \"gamma\": .5,\n",
        "         \"no_cuda\" : False,\n",
        "         \"run_dry\": False,\n",
        "         \"seed\": 0,\n",
        "         \"log_interval\" : 100,\n",
        "         \"dry_run\" : False,\n",
        "         \"save_model\": True}\n",
        "\n",
        "\n",
        "\n",
        "use_cuda = not args[\"no_cuda\"] and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "# torch.manual_seed(args[\"seed\"])\n",
        "\n",
        "\n",
        "# Loading Train / Test Data\n",
        "hs_dataset = Heston_LHS_data_generator(n = 10**5)\n",
        "train_size, test_size = int(hs_dataset.shape[0]*0.9), int(hs_dataset.shape[0]*0.1 ) # 10% SPLIT\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(hs_dataset, [train_size, hs_dataset.shape[0] - train_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, args[\"batch_size\"])\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, args[\"test_batch_size\"])\n",
        "\n",
        "\n",
        "# Model training\n",
        "print(device)\n",
        "model = Heston_ANN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"]) # Adam is found to be the best optimizer in the article\n",
        "scheduler = StepLR(optimizer, step_size=500, gamma=args[\"gamma\"]) # Deacreses the Learning rate by .5 every 500 epoch\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(1, args[\"epochs\"] + 1):\n",
        "  train_loss = model.train_model(args, device, train_loader, optimizer, epoch)\n",
        "  test_loss = model.test_model(device, test_loader)\n",
        "  print(\"Current Learning rate {}\".format(scheduler.get_last_lr()[0]))\n",
        "  scheduler.step()\n",
        "  train_losses.append(train_loss)\n",
        "  test_losses.append(test_loss)\n",
        "if args[\"save_model\"] :\n",
        "  torch.save(model.state_dict(), \"Heston_ANN_DE.pt\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "rEIyI-TaRaCZ",
        "outputId": "16eb6362-1a2b-4bb1-9f60-07947e1de77e"
      },
      "outputs": [],
      "source": [
        "# Plotting log(MSE) as a function of epochs\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot( np.log(test_losses), label = \"test\")\n",
        "plt.plot( np.log(train_losses), label = \"train\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Log(LSE)\")\n",
        "plt.title(\"MSE over Epoch\" )\n",
        "plt.grid()\n",
        "print(\"Training WIDE {} VS in the article 8.04e-09\".format(train_losses[-1]) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQX439O8R4Q3",
        "outputId": "707adfbb-3420-4563-8376-1df000c402c3"
      },
      "outputs": [],
      "source": [
        "# Loading model\n",
        "device = 'cuda'\n",
        "loaded_Heston_ANN = Heston_ANN().to(device)\n",
        "PATH = \"ANN_weights/Heston_ANN_DE.pt\"\n",
        "loaded_Heston_ANN.load_state_dict(torch.load(PATH))\n",
        "loaded_Heston_ANN.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# printing train and test erros MSE, MAE, MAPE, R2.\n",
        "outputs = []\n",
        "targets = []\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
        "    output = loaded_Heston_ANN(data)\n",
        "    outputs.append((output).data.cpu().numpy())\n",
        "    targets.append((target).data.cpu().numpy())\n",
        "  outputs = np.concatenate(outputs)\n",
        "  targets = np.concatenate(targets)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
        "print(\"-----------Train scores----------------\")\n",
        "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
        "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
        "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
        "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )\n",
        "\n",
        "\n",
        "outputs = []\n",
        "targets = []\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
        "    output = loaded_Heston_ANN(data)\n",
        "    outputs.append((output).data.cpu().numpy())\n",
        "    targets.append((target).data.cpu().numpy())\n",
        "  outputs = np.concatenate(outputs)\n",
        "  targets = np.concatenate(targets)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
        "print(\"-----------Test scores----------------\")\n",
        "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
        "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
        "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
        "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data loader \n",
        "loader = torch.utils.data.DataLoader(hs_dataset, 4048)\n",
        "# Computing prices\n",
        "prices = []\n",
        "with torch.no_grad():\n",
        "  for data in loader:\n",
        "    data, _ = data[:,:-1].to(device), data[:,-1].to(device)\n",
        "    output = loaded_Heston_ANN(data)\n",
        "    prices.append((output).data.cpu().numpy())\n",
        "  prices = np.concatenate(prices)\n",
        "\n",
        "print(prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import optimize\n",
        "\n",
        "def calcimpliedvol(dataset):\n",
        "    \"\"\"\n",
        "    Computes the implied volatilities using the brent method.\n",
        "    dataset: numpy array containing moneyness, time to maturity, risk free interest rate and V/K.\n",
        "    Returns a vector of implied volatilities.\n",
        "    \"\"\"\n",
        "    money = np.array(dataset[0])\n",
        "    T = np.array(dataset[1])\n",
        "    r = np.array(dataset[2])\n",
        "    marketoptionPrice = np.array(dataset[3])\n",
        "    def bs_price(sigma):\n",
        "      d1 = ( np.log(money) + (r + 0.5 * sigma ** 2) * (T) ) / (sigma * np.sqrt(T))\n",
        "      d2 = d1 - sigma*np.sqrt(T)\n",
        "      value_moyeness = money * norm.cdf(d1) - np.exp(-r * (T)) * norm.cdf(d2) \n",
        "      fx = value_moyeness - marketoptionPrice\n",
        "      return fx\n",
        "    return optimize.brentq(bs_price,-1,1,maxiter=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eps = 1e-2 # is used to generate open intervals\n",
        "# (S/K), (tau), (r), (rho), (kappa), (vbar), (gamma), (v0).\n",
        "# CASE 1 in table 11\n",
        "#l_bounds = [.75+eps, .4+eps, .0+eps, -0.95+eps, .4+eps, .0+eps, .0+eps, .05+eps]\n",
        "#u_bounds = [1.25-eps, 1.-eps, .1-eps, 0.0-eps, 2.0-eps, .5-eps, .5-eps, .5-eps]\n",
        "hs_dataset = Heston_LHS_data_generator(10**5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data loader \n",
        "loader = torch.utils.data.DataLoader(hs_dataset, 4048)\n",
        "# Computing prices\n",
        "prices = []\n",
        "with torch.no_grad():\n",
        "  for data in loader:\n",
        "    data, _ = data[:,:-1].to(device), data[:,-1].to(device)\n",
        "    output = loaded_Heston_ANN(data)\n",
        "    prices.append((output).data.cpu().numpy())\n",
        "  prices = np.concatenate(prices)\n",
        "\n",
        "# Replace with this\n",
        "hs_dataset2 = hs_dataset.data[:, np.array([0, 1, 2, 8])]  # Select relevant columns\n",
        "hs_dataset2[:, 3] = torch.FloatTensor(prices)  # Use Heston-ANN prices as market prices for Brent's method\n",
        "\n",
        "# Compute implied volatilities using Brent's method\n",
        "implied_vol = np.apply_along_axis(calcimpliedvol, 1, hs_dataset2)\n",
        "\n",
        "# Eliminating Nan values\n",
        "indecies = np.argwhere(np.isnan(implied_vol))\n",
        "implied_vol = np.delete(implied_vol, indecies, 0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(implied_vol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Individiual parameters\n",
        "# 4.1.1 in the paper\n",
        "T = 2 \n",
        "S0 = 100\n",
        "kappa = 0.1\n",
        "gamma = 0.1  # Volatility of variance\n",
        "nu_bar = 0.1\n",
        "rho = -0.75\n",
        "nu_0 = 0.05\n",
        "r = 0.05\n",
        "#\"rho\": (-0.9, 0.0),         # Correlation coefficient\n",
        "##   \"kappa\": (0.1, 3.0),        # Speed of mean reversion\n",
        "#    \"gamma\": (0.01, 0.8),      \n",
        " #   \"nu_0\": (0.01, 0.5),        # Initial variance\n",
        " #   \"nu_bar\": (0.01, 0.5)       # Long-term variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading model \n",
        "device = torch.device(\"cuda\")\n",
        "loaded_IV_ANN = Heston_ANN().to(device)\n",
        "PATH = \"ANN_weights/IV_SCALED_ANN.pt\"\n",
        "loaded_IV_ANN.load_state_dict(torch.load(PATH))\n",
        "loaded_IV_ANN.eval()\n",
        "\n",
        "\n",
        "# Plugging in the Heston-ANN prices\n",
        "hs_dataset2 = hs_dataset.data[:,np.array([0,1,2,8])]\n",
        "hs_dataset2.data[:,3] = torch.FloatTensor(prices)\n",
        "loader2 = torch.utils.data.DataLoader(hs_dataset2, 4048)\n",
        "\n",
        "# Computing IVs\n",
        "implied_vol = []\n",
        "indecies_list = []\n",
        "with torch.no_grad():\n",
        "  for idx, data in enumerate(loader2):\n",
        "    data = data.to(device)\n",
        "    # LOG Transformation\n",
        "    data[:,3] = data[:,3] - torch.maximum(data[:,0]-torch.exp(-data[:,1]*data[:,2]),torch.zeros(data[:,0].shape).to(device))\n",
        "    data[:,3] = torch.log(data[:,3])\n",
        "    output = loaded_IV_ANN(data)\n",
        "    implied_vol.append((output).data.cpu().numpy())\n",
        "  implied_vol_pipe2 = np.concatenate(implied_vol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# Define Heston parameter bounds\n",
        "bounds = {\n",
        "    \"rho\": (-0.9, 0.0),         # Correlation coefficient\n",
        "    \"kappa\": (0.1, 3.0),        # Speed of mean reversion\n",
        "    \"gamma\": (0.01, 0.8),       # Volatility of variance\n",
        "    \"nu_0\": (0.01, 0.5),        # Initial variance\n",
        "    \"nu_bar\": (0.01, 0.5)       # Long-term variance\n",
        "}\n",
        "\n",
        "# Number of discrete points to sample within each parameter's range\n",
        "points_per_param = 5\n",
        "\n",
        "# Generate equally spaced points for each parameter within its bounds\n",
        "parameter_points = {param: np.linspace(lower, upper, points_per_param)\n",
        "                    for param, (lower, upper) in bounds.items()}\n",
        "\n",
        "# Combine the points to form a grid of all possible parameter combinations\n",
        "grid = list(itertools.product(\n",
        "    parameter_points[\"rho\"],\n",
        "    parameter_points[\"kappa\"],\n",
        "    parameter_points[\"gamma\"],\n",
        "    parameter_points[\"nu_0\"],\n",
        "    parameter_points[\"nu_bar\"]\n",
        "))\n",
        "\n",
        "# Convert the grid into a NumPy array for convenience\n",
        "parameter_grid = np.array(grid)\n",
        "\n",
        "# Display the grid details\n",
        "print(f\"Search space dimensions: {len(parameter_points)} parameters\")\n",
        "print(f\"Total combinations: {parameter_grid.shape[0]}\")\n",
        "print(\"Example combinations (first 5 rows):\")\n",
        "print(parameter_grid[:5])\n",
        "\n",
        "# Optional: Save to a file for later use\n",
        "np.savetxt(\"heston_search_space.csv\", parameter_grid, delimiter=\",\",\n",
        "           header=\"rho,kappa,gamma,nu_0,nu_bar\", comments=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Observed market implied volatilities (target data)\n",
        "\n",
        "observed_vols = np.linspace(0.2, 0.5, 35)  # Generates 35 equally spaced values\n",
        "\n",
        "# Moneyness and time-to-maturity of the observed data\n",
        "moneyness = np.linspace(0.85, 1.15, 5)  # Moneyness\n",
        "maturities = np.linspace(0.5, 2.0, 7)   # Time to maturity\n",
        "r = 0.03\n",
        "#params = \n",
        "\n",
        "\n",
        "\n",
        "# Pre-trained ANN that maps Heston parameters to implied volatilities\n",
        "def heston_ann(parameters):\n",
        "    \"\"\"\n",
        "    A mock function representing the ANN (pre-trained forward pass).\n",
        "    In real scenarios, this will return ANN-predicted implied volatilities.\n",
        "    \"\"\"\n",
        "    # Example: Simple mapping for illustration\n",
        "    rho, kappa, gamma, nu_bar, nu_0 = parameters\n",
        "    return 0.25 + 0.05 * (moneyness - 1.0) + 0.02 * (maturities - 1.0)\n",
        "\n",
        "# Objective function for DE calibration\n",
        "def calibration_objective(parameters):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared error between ANN-predicted\n",
        "    and observed market volatilities.\n",
        "    \"\"\"\n",
        "    predicted_vols = implied_vol_pipe2\n",
        "    error = np.mean((predicted_vols - observed_vols) ** 2)\n",
        "    return error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bounds = [\n",
        "    (-0.75, -0.25),   # rho (correlation)\n",
        "    (0.5, 1.0),    # kappa (mean reversion speed)\n",
        "    (0.3, 0.5),   # gamma (volatility of variance)\n",
        "    (0.15, 0.35),   # nu_bar (long-run variance)\n",
        "    (0.5, 1.0)    # nu_0 (initial variance)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.optimize import differential_evolution\n",
        "\n",
        "# Run DE to calibrate the Heston parameters\n",
        "result = differential_evolution(\n",
        "    calibration_objective,  # Objective function\n",
        "    bounds,                 # Parameter bounds\n",
        "    strategy=\"best1bin\",    # Strategy for mutation\n",
        "    popsize=50,             # Population size\n",
        "    mutation=(0.5, 1.0),    # Mutation range\n",
        "    recombination=0.7,      # Crossover probability\n",
        "    tol=0.01,               # Convergence tolerance\n",
        "    disp=True               # Display optimization progress\n",
        ")\n",
        "\n",
        "# Display calibrated parameters\n",
        "print(\"Calibrated Parameters:\", result.x)\n",
        "print(\"Objective Function Value:\", result.fun)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate implied volatility using Brent's method\n",
        "def calc_implied_vol(dataset):\n",
        "    \"\"\"\n",
        "    Computes implied volatilities using the Brent method.\n",
        "    :param dataset: numpy array containing [S/K, T, r, OptionPrice].\n",
        "    :return: A single implied volatility value.\n",
        "    \"\"\"\n",
        "    money, T, r, market_price = dataset\n",
        "\n",
        "    def bs_price(sigma):\n",
        "        \"\"\"Black-Scholes option price difference function.\"\"\"\n",
        "        d1 = (np.log(money) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
        "        d2 = d1 - sigma * np.sqrt(T)\n",
        "        theoretical_price = money * norm.cdf(d1) - np.exp(-r * T) * norm.cdf(d2)\n",
        "        return theoretical_price - market_price\n",
        "\n",
        "    # Solve for implied volatility\n",
        "    try:\n",
        "        return optimize.brentq(bs_price, 1e-5, 5, maxiter=100)\n",
        "    except ValueError:\n",
        "        return np.nan  # Return NaN if Brent fails to converge\n",
        "\n",
        "# Data generation or loading\n",
        "hs_dataset = Heston_LHS_data_generator(10**5, l_bounds=l_bounds, u_bounds=u_bounds)\n",
        "hs_data_loader = DataLoader(hs_dataset, batch_size=4048)\n",
        "\n",
        "# Step 1: Compute prices using Heston-ANN\n",
        "predicted_prices = []\n",
        "with torch.no_grad():\n",
        "    for data in hs_data_loader:\n",
        "        inputs = data[:, :-1].to(device)  # Exclude the last column if it's labels\n",
        "        outputs = loaded_Heston_ANN(inputs)\n",
        "        predicted_prices.append(outputs.cpu().numpy())\n",
        "predicted_prices = np.concatenate(predicted_prices)\n",
        "\n",
        "# Step 2: Prepare the dataset for Brent's method\n",
        "# Combine predicted prices with necessary inputs for implied volatility calculation\n",
        "# Assuming hs_dataset.data contains [S/K, T, r, actual_price]:\n",
        "hs_dataset_data = hs_dataset.data[:, np.array([0, 1, 2])]  # Extract relevant columns\n",
        "hs_dataset_data = np.hstack((hs_dataset_data, predicted_prices.reshape(-1, 1)))\n",
        "\n",
        "# Step 3: Calculate implied volatilities\n",
        "implied_volatilities = np.apply_along_axis(calc_implied_vol, 1, hs_dataset_data)\n",
        "\n",
        "# Step 4: Evaluate results\n",
        "# Assuming you have ground-truth implied volatilities for comparison\n",
        "ground_truth_vol = hs_dataset.data[:, 3]  # Replace with actual column for ground truth\n",
        "mse = ((ground_truth_vol - implied_volatilities) ** 2).mean()\n",
        "print(f\"MSE: {mse:.6e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ0JXu7pjTIb"
      },
      "outputs": [],
      "source": [
        "# Loading Train / Test Data\n",
        "hs_dataset = Heston_LHS_data_generator(n = 10**5)\n",
        "train_size, test_size = int(hs_dataset.shape[0]*0.9), int(hs_dataset.shape[0]*0.1 ) # 10% SPLIT\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(hs_dataset, [train_size, hs_dataset.shape[0]-train_size])\n",
        "args =  {\"batch_size\": 1024,\n",
        "         \"test_batch_size\": 4048,\n",
        "         \"epochs\" : 3*10**3,\n",
        "         \"lr\": 1e-4,\n",
        "         \"gamma\": .1,\n",
        "         \"no_cuda\" : False,\n",
        "         \"run_dry\": False,\n",
        "         \"seed\": 0,\n",
        "         \"log_interval\" : 100,\n",
        "         \"dry_run\" : False,\n",
        "         \"save_model\": True}\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,args[\"batch_size\"])\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, args[\"test_batch_size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdy_UxgmRiVo",
        "outputId": "72a26728-24c6-4440-a6f5-2b766606dbfb"
      },
      "outputs": [],
      "source": [
        "# printing train and test erros MSE, MAE, MAPE, R2.\n",
        "outputs = []\n",
        "targets = []\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
        "    output = loaded_Heston_ANN(data)\n",
        "    outputs.append((output).data.cpu().numpy())\n",
        "    targets.append((target).data.cpu().numpy())\n",
        "  outputs = np.concatenate(outputs)\n",
        "  targets = np.concatenate(targets)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
        "print(\"-----------Train scores----------------\")\n",
        "print(\"MSE {:e}\".format(mean_squared_error(outputs, targets)) )\n",
        "print(\"MAE {:e}\".format(mean_absolute_error(outputs, targets)) )\n",
        "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs, targets)) )\n",
        "print(\"R2 {:e}\".format(r2_score(outputs, targets)) )\n",
        "\n",
        "\n",
        "outputs = []\n",
        "targets = []\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    data, target = data[:,:-1].to(device), data[:,-1].to(device)\n",
        "    output = loaded_Heston_ANN(data)\n",
        "    outputs.append((output).data.cpu().numpy())\n",
        "    targets.append((target).data.cpu().numpy())\n",
        "  outputs = np.concatenate(outputs)\n",
        "  targets = np.concatenate(targets)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
        "print(\"-----------Test scores----------------\")\n",
        "print(\"MSE {:e}\".format(mean_squared_error(outputs, targets)) )\n",
        "print(\"MAE {:e}\".format(mean_absolute_error(outputs, targets)) )\n",
        "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs, targets)) )\n",
        "print(\"R2 {:e}\".format(r2_score(outputs, targets)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Boa8FA_ZoSuw",
        "outputId": "faa492fa-c559-4a5a-a950-0c0fdb4d45d9"
      },
      "outputs": [],
      "source": [
        "np.sqrt(1.670718e-07)\n",
        "print(\"MSE {:e}\".format(np.sqrt(1.670718e-07)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2J3aEAhSflZ"
      },
      "source": [
        "# 5) BRENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA3X7W_KAhSr"
      },
      "source": [
        "# 6) Pipeline 1: COS + BRENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhQUeGPqAmt1",
        "outputId": "69defb04-1caa-4a6a-d307-f25ed0be81e2"
      },
      "outputs": [],
      "source": [
        "eps = 1e-2 # is used to generate open intervals\n",
        "# (S/K), (tau), (r), (rho), (kappa), (vbar), (gamma), (v0).\n",
        "# CASE 1 in table 11\n",
        "l_bounds = [.75+eps, .4+eps, .0+eps, -0.95+eps, .4+eps, .0+eps, .0+eps, .05+eps]\n",
        "u_bounds = [1.25-eps, 1.0-eps, .1-eps, 0.0-eps, 2.0-eps, .5-eps, .5-eps, .5-eps]\n",
        "hs_dataset = Heston_LHS_data_generator(10**5, l_bounds = l_bounds, u_bounds=u_bounds)\n",
        "hs_dataset_data = hs_dataset.data\n",
        "hs_dataset_data = hs_dataset_data[:,np.array([0,1,2,8])]\n",
        "implied_vol_pipe1 = np.apply_along_axis(calcimpliedvol, 1, hs_dataset_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzV98IpxAm3_"
      },
      "source": [
        "# 7) Pipeline 2: Heston-ANN + IV-ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrch0eIxAszs"
      },
      "outputs": [],
      "source": [
        "# Loading model\n",
        "device = torch.device(\"cuda\")\n",
        "loaded_Heston_ANN = Heston_ANN().to(device)\n",
        "PATH = \"ANN_weights/Heston_ANN.pt\"\n",
        "loaded_Heston_ANN.load_state_dict(torch.load(PATH))\n",
        "loaded_Heston_ANN.eval()\n",
        "\n",
        "# data loader \n",
        "loader = torch.utils.data.DataLoader(hs_dataset, 4048)\n",
        "# Computing prices\n",
        "prices = []\n",
        "with torch.no_grad():\n",
        "  for data in loader:\n",
        "    data, _ = data[:,:-1].to(device), data[:,-1].to(device)\n",
        "    output = loaded_Heston_ANN(data)\n",
        "    prices.append((output).data.cpu().numpy())\n",
        "  prices = np.concatenate(prices)\n",
        "\n",
        "# Loading model \n",
        "device = torch.device(\"cuda\")\n",
        "loaded_IV_ANN = IV_ANN().to(device)\n",
        "PATH = \"ANN_weights/IV_SCALED_ANN.pt\"\n",
        "loaded_IV_ANN.load_state_dict(torch.load(PATH))\n",
        "loaded_IV_ANN.eval()\n",
        "\n",
        "\n",
        "# Plugging in the Heston-ANN prices\n",
        "hs_dataset2 = hs_dataset.data[:,np.array([0,1,2,8])]\n",
        "hs_dataset2.data[:,3] = torch.FloatTensor(prices)\n",
        "loader2 = torch.utils.data.DataLoader(hs_dataset2, 4048)\n",
        "\n",
        "# Computing IVs\n",
        "implied_vol = []\n",
        "indecies_list = []\n",
        "with torch.no_grad():\n",
        "  for idx, data in enumerate(loader2):\n",
        "    data = data.to(device)\n",
        "    # LOG Transformation\n",
        "    data[:,3] = data[:,3] - torch.maximum(data[:,0]-torch.exp(-data[:,1]*data[:,2]),torch.zeros(data[:,0].shape).to(device))\n",
        "    data[:,3] = torch.log(data[:,3])\n",
        "    output = loaded_IV_ANN(data)\n",
        "    implied_vol.append((output).data.cpu().numpy())\n",
        "  implied_vol_pipe2 = np.concatenate(implied_vol)\n",
        "  # indecies_list = np.concatenate(indecies_list).flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmdMT4oKvBvO"
      },
      "outputs": [],
      "source": [
        "# Eliminating Nan values resulting from prices lower than 1e-7\n",
        "indecies = np.argwhere(np.isnan(implied_vol_pipe2))\n",
        "implied_vol_pipe1 = np.delete(implied_vol_pipe1, indecies,0)\n",
        "implied_vol_pipe2 = np.delete(implied_vol_pipe2, indecies,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKD1CeSZkxcO"
      },
      "source": [
        "# 8) Pipeline 1 VS pipeline 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6heX_6z9gaRF",
        "outputId": "8423e860-9186-4273-abe4-6c7393b8da26"
      },
      "outputs": [],
      "source": [
        "# CASE 1\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
        "outputs = implied_vol_pipe2\n",
        "targets = implied_vol_pipe1\n",
        "#targets = vol_imp\n",
        "print(\"-----------Vol scores----------------\")\n",
        "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
        "print(\"RMSE {:e}\".format(np.sqrt(mean_squared_error(outputs,targets))) )\n",
        "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
        "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
        "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmRj5W23v81E",
        "outputId": "e43a6959-6354-4853-893d-772c31abdf2f"
      },
      "outputs": [],
      "source": [
        "# CASE 2\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
        "outputs = implied_vol_pipe2\n",
        "targets = implied_vol_pipe1\n",
        "print(\"-----------Vol scores----------------\")\n",
        "print(\"MSE {:e}\".format(mean_squared_error(outputs,targets)) )\n",
        "print(\"RMSE {:e}\".format(np.sqrt(mean_squared_error(outputs,targets))) )\n",
        "print(\"MAE {:e}\".format(mean_absolute_error(outputs,targets)) )\n",
        "print(\"MAPE {:e}\".format(mean_absolute_percentage_error(outputs,targets)) )\n",
        "print(\"R2 {:e}\".format(r2_score(outputs,targets)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLwRkWB09WAh"
      },
      "source": [
        "# 9) Volatility surface (COS + IV-ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYbO_aqdEcyQ"
      },
      "source": [
        "### Initial volatility surface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOh3UfV4gdbm"
      },
      "outputs": [],
      "source": [
        "# Generating the surface points\n",
        "r= .02\n",
        "tau= 0.5\n",
        "rho = -0.05\n",
        "kappa= 1.5\n",
        "vbar = .1\n",
        "gamma = .3\n",
        "v0 = 0.1\n",
        "moyeness = 0.74\n",
        "moyeness = np.linspace(moyeness, 1.24,11)\n",
        "tau = np.linspace(tau, 1, 6)\n",
        "\n",
        "cos_prices = []\n",
        "X = []\n",
        "Y = []\n",
        "for m in moyeness:\n",
        "  for t in tau:\n",
        "    X.append(m)\n",
        "    Y.append(t)\n",
        "    cos_prices.append(call_option_Heston_moy(N = 160, L= 12, r= r, tau= t, kappa= kappa, gamma = gamma, vbar = vbar, v0 = v0, rho = rho, moyeness = m))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_vzIM26hchq"
      },
      "outputs": [],
      "source": [
        "# Reshaping for torch tensor dataset\n",
        "cos_prices = np.array(cos_prices).reshape((-1, 1))\n",
        "X = np.array(X).reshape((-1, 1))\n",
        "Y = np.array(Y).reshape((-1, 1))\n",
        "r = np.array(r*np.ones(66)).reshape((-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB2nF_gWD10-"
      },
      "outputs": [],
      "source": [
        "# Creating a torch dataset\n",
        "data = np.concatenate((X, Y, r, cos_prices), axis = 1)\n",
        "data = torch.FloatTensor(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmE2kJOhDlzY"
      },
      "outputs": [],
      "source": [
        "# Getting the implied volatility from the dataset\n",
        "with torch.no_grad():\n",
        "  data = data.to(device)\n",
        "  data[:,3] = data[:,3] - torch.maximum(data[:,0]-torch.exp(-data[:,1]*data[:,2]),torch.zeros(data[:,0].shape).to(device))\n",
        "  data[:,3] = torch.log(data[:,3])\n",
        "  output = loaded_IV_ANN(data)\n",
        "  bs_IV = (output).data.cpu().numpy()\n",
        "\n",
        "X = X.flatten()\n",
        "Y = Y.flatten()\n",
        "Z = bs_IV\n",
        "\n",
        "x = np.reshape(X, (-1, 6))\n",
        "y = np.reshape(Y, (-1, 6))\n",
        "z = np.reshape(Z, (-1, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn3cqcg1FCaM"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "from matplotlib import cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "gpj28yIi1SO2",
        "outputId": "83479b92-a03d-4f69-f41f-8bc0308b94db"
      },
      "outputs": [],
      "source": [
        "# Plotting the initial volatility surface\n",
        "plt.figure(figsize=(10,10))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.view_init(elev=20, azim=60)\n",
        "ax.plot_surface(x, y, z,cmap='viridis', edgecolor='none', antialiased=True)\n",
        "ax.set_xlabel('Moyeness')\n",
        "ax.set_ylabel('Time to matuirity')\n",
        "ax.set_zlabel('Implied volatility')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRyQIRUOEYa0"
      },
      "source": [
        "### Smoothing the volatility surface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlUbJTC2253E"
      },
      "outputs": [],
      "source": [
        "from scipy import interpolate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "AWU-1yMd254K",
        "outputId": "78f0535d-e9b0-44eb-db8b-70645b44829c"
      },
      "outputs": [],
      "source": [
        "# Smoothing the volatility surface\n",
        "xnew, ynew = x.copy(), y.copy()\n",
        "tck = interpolate.bisplrep(x, y, z, s=1.0)\n",
        "znew = interpolate.bisplev(xnew[:,0], ynew[0,:], tck)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.view_init(elev=20, azim=60)\n",
        "ax.plot_surface(xnew, ynew, znew,cmap='viridis', edgecolor='none', antialiased=True)\n",
        "ax.set_xlabel('Moyeness')\n",
        "ax.set_ylabel('Time to maturity')\n",
        "ax.set_zlabel('Implied volatility')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fo8DWjnxYK4"
      },
      "source": [
        "### Implied volatility difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8x5s7Zk2tTE"
      },
      "outputs": [],
      "source": [
        "# Generating the surface points\n",
        "r= .02\n",
        "tau= 0.5\n",
        "rho = -0.05\n",
        "kappa= 1.5\n",
        "vbar = .1\n",
        "gamma = .3\n",
        "v0 = 0.1\n",
        "moyeness = 0.74\n",
        "n_moneyness = 11\n",
        "n_tau = 6\n",
        "moyeness = np.linspace(moyeness, 1.24,n_moneyness)\n",
        "tau = np.linspace(tau, 1, n_tau)\n",
        "\n",
        "cos_prices = []\n",
        "X = []\n",
        "Y = []\n",
        "for m in moyeness:\n",
        "  for t in tau:\n",
        "    X.append(m)\n",
        "    Y.append(t)\n",
        "    cos_prices.append(call_option_Heston_moy(N = 160, L= 12, r= r, tau= t, kappa= kappa, gamma = gamma, vbar = vbar, v0 = v0, rho = rho, moyeness = m))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmoYE-ad2wS_"
      },
      "outputs": [],
      "source": [
        "rho = np.array(rho*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
        "kappa = np.array(kappa*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
        "vbar = np.array(vbar*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
        "gamma = np.array(gamma*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
        "v0 = np.array(v0*np.ones(n_tau * n_moneyness)).reshape((-1, 1))\n",
        "# Reshaping for torch tensor dataset\n",
        "cos_prices = np.array(cos_prices).reshape((-1, 1))\n",
        "X = np.array(X).reshape((-1, 1))\n",
        "Y = np.array(Y).reshape((-1, 1))\n",
        "r = np.array(r*np.ones(n_tau * n_moneyness)).reshape((-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2wKtc9r22gK"
      },
      "outputs": [],
      "source": [
        "data_ann = np.concatenate((X, Y, r, rho, kappa, vbar, gamma, v0, cos_prices), axis = 1)\n",
        "data_ann = torch.FloatTensor(data_ann)\n",
        "# Creating a torch dataset\n",
        "data = np.concatenate((X, Y, r, cos_prices), axis = 1)\n",
        "data = torch.FloatTensor(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWoZ5J34xc_v"
      },
      "outputs": [],
      "source": [
        "implied_vol_pipe1 = np.apply_along_axis(calcimpliedvol, 1, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Foh8xvsN3Jy2"
      },
      "outputs": [],
      "source": [
        "# Getting the implied volatility from the dataset\n",
        "with torch.no_grad():\n",
        "  data = data.to(device)\n",
        "  data[:,3] = data[:,3] - torch.maximum(data[:,0]-torch.exp(-data[:,1]*data[:,2]),torch.zeros(data[:,0].shape).to(device))\n",
        "  data[:,3] = torch.log(data[:,3])\n",
        "  output = loaded_IV_ANN(data)\n",
        "  implied_vol_pipe2 = (output).data.cpu().numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "qlJw_Wg91h2Z",
        "outputId": "4dcb6936-f76a-4351-f28d-9dd2b33cb859"
      },
      "outputs": [],
      "source": [
        "X = X.flatten()\n",
        "Y = Y.flatten()\n",
        "Z = (implied_vol_pipe1 - implied_vol_pipe2).flatten()\n",
        "\n",
        "x = np.reshape(X, (-1, min(n_moneyness, n_tau)))\n",
        "y = np.reshape(Y, (-1, min(n_moneyness, n_tau)))\n",
        "z = np.reshape(Z, (-1, min(n_moneyness, n_tau)))\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pcolormesh(x, y, z, shading='gouraud')\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Moneyness\")\n",
        "plt.ylabel(\"Time to maturity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-import necessary libraries and re-define the setup after reset\n",
        "from scipy.optimize import differential_evolution\n",
        "import numpy as np\n",
        "\n",
        "# Define the objective function\n",
        "def objective_function(x):\n",
        "    # Example: A simple quadratic function\n",
        "    return x[0]**2 + x[1]**2 + x[2]**2\n",
        "\n",
        "# Define the bounds for each variable\n",
        "bounds = [(-10, 10), (-10, 10), (-10, 10)]  # Variable bounds for a 3D problem\n",
        "\n",
        "# Run Differential Evolution with the specified settings\n",
        "result_custom_de = differential_evolution(\n",
        "    objective_function,  # Objective function\n",
        "    bounds,             # Variable bounds\n",
        "    strategy=\"best1bin\",  # Strategy\n",
        "    popsize=50,          # Population size\n",
        "    mutation=(0.5, 1.0), # Mutation range\n",
        "    recombination=0.7,   # Crossover probability\n",
        "    tol=0.01,            # Convergence tolerance\n",
        "    disp=False           # Suppress output during the run\n",
        ")\n",
        "\n",
        "# Display the result\n",
        "result_custom_de\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ll-jOXXHagp0",
        "CdhtgoThB8B3",
        "yNTO4M51R3cd",
        "z_OVhr6nR-Iz",
        "aWu772JpMRaZ",
        "Glh6JAzfpWHm",
        "lTZo9gF1GJwq",
        "dGGuqkhpzjak",
        "SW3NslRmKsTU",
        "1EImNgxzAbeI",
        "78q0DF6HAeN5",
        "c3QMaOLPPwux",
        "fBQ-dVz4DFUh",
        "py6auAo1DAq3",
        "oF6h1RohC9KX",
        "4fO7JQlDDNcW",
        "ehFOGXCpDPyX",
        "-aCVlS1x_RIt",
        "AOjnH0N6HMXt",
        "C3o5w0J_Hl1O",
        "F2J3aEAhSflZ",
        "DA3X7W_KAhSr",
        "pzV98IpxAm3_",
        "BKD1CeSZkxcO",
        "KLwRkWB09WAh",
        "oYbO_aqdEcyQ",
        "lRyQIRUOEYa0",
        "5fo8DWjnxYK4"
      ],
      "name": "Pytorch_Project_HIZAOUI_MASSONI_MEKRAMI.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ANN_Solver",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
